#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ (ìµœì í™” ë²„ì „)
- RSS ìˆ˜ì§‘ â†’ ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ â†’ ë‹¨ê³„ë³„ AI ì •ì œ
- ë¬´ë£Œ API (Gemini, Groq) + ìœ ë£Œ API (OpenAI, Claude) ì¡°í•©
"""

import os
import sys
import json
import csv
import feedparser
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import time
import logging
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / 'data'
SCRIPTS_DIR = PROJECT_ROOT / 'scripts'
KEYWORDS_FILE = SCRIPTS_DIR / 'keywords.json'
NEWS_CSV = DATA_DIR / 'news.csv'
CACHE_FILE = DATA_DIR / 'news_cache.json'

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ë¡œì»¬ ê°œë°œ í™˜ê²½)
try:
    from dotenv import load_dotenv
    load_dotenv(PROJECT_ROOT / '.env')
    logger.info("ë¡œì»¬ í™˜ê²½: .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ")
except ImportError:
    logger.info("GitHub Actions í™˜ê²½: os.getenv ì‚¬ìš©")


# ============================================================
# API ìš°ì„ ìˆœìœ„ ë° ë¹„ìš© ì„¤ì •
# ============================================================
API_PRIORITY = {
    'free': ['groq', 'gemini'],  # ë¬´ë£Œ API ìš°ì„ 
    'paid': ['openai', 'claude']  # ìœ ë£Œ APIëŠ” ì„ íƒì 
}

API_COSTS = {
    'groq': 0,       # ë¬´ë£Œ
    'gemini': 0,     # ë¬´ë£Œ í‹°ì–´
    'openai': 0.0001,  # GPT-4o-mini per request
    'claude': 0.003    # Claude per request
}


def load_keywords() -> Dict:
    """í‚¤ì›Œë“œ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    try:
        with open(KEYWORDS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.error(f"í‚¤ì›Œë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {KEYWORDS_FILE}")
        return {
            "base_keywords": ["PUBG Mobile"],
            "country_keywords": {},
            "categories": ["gaming"]
        }
    except json.JSONDecodeError:
        logger.error("í‚¤ì›Œë“œ íŒŒì¼ JSON íŒŒì‹± ì˜¤ë¥˜")
        return {"base_keywords": ["PUBG Mobile"], "country_keywords": {}, "categories": ["gaming"]}


def get_continent(country: str) -> str:
    """êµ­ê°€ëª…ìœ¼ë¡œ ëŒ€ë¥™ ë°˜í™˜"""
    continent_map = {
        'USA': 'NORTH AMERICA', 'Canada': 'NORTH AMERICA', 'Mexico': 'NORTH AMERICA',
        'Brazil': 'SOUTH AMERICA', 'Argentina': 'SOUTH AMERICA',
        'Germany': 'EUROPE', 'UK': 'EUROPE', 'France': 'EUROPE', 'Italy': 'EUROPE', 'Spain': 'EUROPE',
        'China': 'ASIA', 'India': 'ASIA', 'Japan': 'ASIA', 'Korea': 'ASIA', 'South Korea': 'ASIA',
        'South Africa': 'AFRICA', 'Egypt': 'AFRICA', 'Nigeria': 'AFRICA',
        'Australia': 'OCEANIA', 'New Zealand': 'OCEANIA',
        'Russia': 'RUSSIA & CIS'
    }
    return continent_map.get(country, 'OTHER')


def map_to_group_category(detail_category: str) -> str:
    """
    ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë¥¼ ê·¸ë£¹ ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘
    
    Args:
        detail_category: ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: internet_shutdown, war_conflict ë“±)
    
    Returns:
        ê·¸ë£¹ ì¹´í…Œê³ ë¦¬ (outage_block, social_crisis, seasonal_calendar, gaming_competitor, other)
    """
    # ğŸ”´ ì¥ì•  ë° ì°¨ë‹¨ (Outage & Block)
    outage_block = [
        'internet_shutdown', 'tech_outage', 'power_outage', 'censorship',
        'cyber_attack', 'infrastructure_damage'
    ]
    
    # ğŸŸ  ì‚¬íšŒì  ìœ„ê¸° (Social Crisis)
    social_crisis = [
        'war_conflict', 'terrorism_explosion', 'natural_disaster',
        'protest_strike', 'curfew', 'pandemic', 'economic'
    ]
    
    # ğŸŸ¢ ì‹œì¦Œ ë° ì¼ì • (Seasonal & Calendar)
    seasonal_calendar = [
        'holiday', 'school_calendar', 'election'
    ]
    
    # ğŸ”µ ê²Œì„ ë° ê²½ìŸ (Gaming & Competitor)
    gaming_competitor = [
        'gaming', 'competitor_game', 'social_trend', 'sports_event', 'major_event'
    ]
    
    if detail_category in outage_block:
        return 'outage_block'
    elif detail_category in social_crisis:
        return 'social_crisis'
    elif detail_category in seasonal_calendar:
        return 'seasonal_calendar'
    elif detail_category in gaming_competitor:
        return 'gaming_competitor'
    else:
        return 'other'


# ============================================================
# ìºì‹± ì‹œìŠ¤í…œ
# ============================================================

def get_cache_key(text: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ ìºì‹œ í‚¤ ìƒì„±"""
    return hashlib.md5(text.encode()).hexdigest()[:16]


def load_cache() -> Dict:
    """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
    try:
        if CACHE_FILE.exists():
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return {}


def save_cache(cache: Dict):
    """ìºì‹œ íŒŒì¼ ì €ì¥"""
    try:
        DATA_DIR.mkdir(exist_ok=True)
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")


# ============================================================
# ë¬´ë£Œ API: Groq (Llama 3.1)
# ============================================================

def fetch_from_groq(news_items: List[Dict], batch_size: int = 5) -> List[Dict]:
    """
    Groq APIë¡œ ë‰´ìŠ¤ ë°°ì¹˜ ë¶„ì„ (ë¬´ë£Œ, ì´ˆê³ ì†)
    - Llama 3.1 70B ì‚¬ìš©
    - ë¶„ë‹¹ 30íšŒ, ì¼ 14,400íšŒ ë¬´ë£Œ
    
    API í‚¤ ë°œê¸‰: https://console.groq.com/
    """
    api_key = os.getenv('GROQ_API_KEY')
    if not api_key:
        logger.info("GROQ_API_KEY ì—†ìŒ - Groq ìŠ¤í‚µ")
        return news_items
    
    try:
        import requests
        
        results = []
        for i in range(0, len(news_items), batch_size):
            batch = news_items[i:i+batch_size]
            
            # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
            news_text = "\n".join([
                f"{j+1}. ì œëª©: {item.get('title', '')}\n   ìš”ì•½: {item.get('summary', '')[:200]}"
                for j, item in enumerate(batch)
            ])
            
            prompt = f"""ë‹¤ìŒ {len(batch)}ê°œ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. ê° ë‰´ìŠ¤ì— ëŒ€í•´ JSON ë°°ì—´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.

{news_text}

ê° ë‰´ìŠ¤ì— ëŒ€í•´:
- category: ì¹´í…Œê³ ë¦¬ (gaming, holiday, war_conflict, natural_disaster, internet_shutdown, protest_strike, economic, other ì¤‘ í•˜ë‚˜)
- traffic_impact: ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ (í•œêµ­ì–´ë¡œ 1-2ë¬¸ì¥)
- relevant: ê´€ë ¨ì„± (true/false)

JSON ë°°ì—´ë§Œ ì‘ë‹µí•˜ì„¸ìš”:
[{{"id": 1, "category": "...", "traffic_impact": "...", "relevant": true}}, ...]"""

            response = requests.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "llama-3.1-70b-versatile",
                    "messages": [
                        {"role": "system", "content": "You are a news analyst. Return only valid JSON array."},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.3,
                    "max_tokens": 1000
                },
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                content = data['choices'][0]['message']['content']
                
                # JSON ì¶”ì¶œ
                import re
                json_match = re.search(r'\[[\s\S]*\]', content)
                if json_match:
                    analysis = json.loads(json_match.group())
                    
                    for j, item in enumerate(batch):
                        if j < len(analysis):
                            item['category'] = analysis[j].get('category', item.get('category', 'other'))
                            item['traffic_impact'] = analysis[j].get('traffic_impact', '')
                            item['api_source'] = 'groq'
                            if not analysis[j].get('relevant', True):
                                item['skip'] = True
                        results.append(item)
                else:
                    results.extend(batch)
            else:
                logger.warning(f"Groq API ì˜¤ë¥˜: {response.status_code}")
                results.extend(batch)
            
            time.sleep(0.5)  # Rate limit ë°©ì§€
        
        logger.info(f"Groq ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ")
        return results
        
    except Exception as e:
        logger.error(f"Groq API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return news_items


# ============================================================
# ë„¤ì´ë²„ ê²€ìƒ‰ API (êµ­ë‚´ ë‰´ìŠ¤) - ì—„ê²©í•œ í•„í„°ë§ ì ìš©
# ============================================================

# ê´‘ê³ /ë§ˆì¼€íŒ…/ê´€ë ¨ì—†ëŠ” ë‰´ìŠ¤ ì œì™¸ í‚¤ì›Œë“œ
NEGATIVE_KEYWORDS = [
    # ë§ˆì¼€íŒ…/ê´‘ê³ 
    'ìº í˜ì¸', 'í”„ë¡œëª¨ì…˜', 'ì´ë²¤íŠ¸', 'ì¶œì‹œ', 'ì‹ ì œí’ˆ', 'í• ì¸', 'ì„¸ì¼', 'íŒì—…', 'ì½œë¼ë³´',
    'campaign', 'promotion', 'launch', 'sale', 'popup', 'collaboration',
    # ì—°ì˜ˆ/ì—”í„°
    'ê±¸ê·¸ë£¹', 'ë³´ì´ê·¸ë£¹', 'ì•„ì´ëŒ', 'ì½˜ì„œíŠ¸', 'ì•¨ë²”', 'ë®¤ì§ë¹„ë””ì˜¤', 'íŒ¬ë¯¸íŒ…',
    # ìŒì‹/ë¸Œëœë“œ
    'ë˜í‚¨', 'ìŠ¤íƒ€ë²…ìŠ¤', 'ë§¥ë„ë‚ ë“œ', 'ë²„ê±°í‚¹', 'ì— ì•¤ì— ', 'ì´ˆì½œë¦¿', 'ì»¤í”¼',
    # ê¸°íƒ€ ë¹„ê´€ë ¨
    'íŒ¨ì…˜', 'ë·°í‹°', 'í™”ì¥í’ˆ', 'ì˜ë¥˜', 'ì‡¼í•‘'
]

# ê²Œì„ ê´€ë ¨ í•„ìˆ˜ í‚¤ì›Œë“œ (PUBGM ê´€ë ¨/ê²½ìŸ ê²Œì„ë§Œ)
GAMING_REQUIRED_KEYWORDS = [
    # PUBG ì§ì ‘ ê´€ë ¨
    'pubg', 'íì§€', 'ë°°í‹€ê·¸ë¼ìš´ë“œ', 'í¬ë˜í”„í†¤', 'krafton', 'bgmi',
    'pmgc', 'pmpl', 'pcs', 'pgc',  # PUBG ëŒ€íšŒ
    
    # ì¥ë¥´ (FPS/ìŠˆí„°/ë°°í‹€ë¡œì–„)
    'fps', 'fpsê²Œì„', 'fps ê²Œì„', 'ìŠˆí„°', 'shooter',
    'ë°°í‹€ë¡œì–„', 'battle royale', 'ë°°í‹€ ë¡œì–„',
    
    # ê²½ìŸ ëª¨ë°”ì¼ ê²Œì„
    'free fire', 'í”„ë¦¬íŒŒì´ì–´', 'ê°€ë ˆë‚˜', 'garena',
    'call of duty mobile', 'cod mobile', 'ì½œì˜¤ë¸Œë“€í‹° ëª¨ë°”ì¼',
    'apex legends mobile', 'ì—ì´í™ìŠ¤ ë ˆì „ë“œ ëª¨ë°”ì¼',
    'fortnite mobile', 'í¬íŠ¸ë‚˜ì´íŠ¸ ëª¨ë°”ì¼',
    
    # ê²½ìŸ í”Œë«í¼/ê²Œì„
    'roblox', 'ë¡œë¸”ë¡ìŠ¤',
    'fortnite', 'í¬íŠ¸ë‚˜ì´íŠ¸',
    'apex legends', 'ì—ì´í™ìŠ¤ ë ˆì „ë“œ', 'ì—ì´í™ìŠ¤',
    
    # ëª¨ë°”ì¼ ê²Œì„ eìŠ¤í¬ì¸ 
    'mobile esports', 'ëª¨ë°”ì¼ eìŠ¤í¬ì¸ ', 'ëª¨ë°”ì¼ ì´ìŠ¤í¬ì¸ '
]

# íŠ¸ë˜í”½ ì˜í–¥ í•„ìˆ˜ í‚¤ì›Œë“œ (ì‹¤ì œ ì˜í–¥ì„ ì£¼ëŠ” ì´ë²¤íŠ¸ë§Œ)
TRAFFIC_IMPACT_KEYWORDS = {
    'disaster': ['ì§€ì§„ ë°œìƒ', 'ì§€ì§„ í”¼í•´', 'íƒœí’ ìƒë¥™', 'íƒœí’ í”¼í•´', 'í™ìˆ˜ í”¼í•´', 'í­ìš° í”¼í•´',
                 'earthquake hit', 'typhoon damage', 'flood damage'],
    'conflict': ['ì „ìŸ ë°œë°œ', 'êµ°ì‚¬ ì¶©ëŒ', 'í­íƒ„ í…ŒëŸ¬', 'ë¬´ë ¥ ì¶©ëŒ', 'ë¯¸ì‚¬ì¼ ê³µê²©',
                 'war outbreak', 'military conflict', 'bombing', 'missile attack'],
    'outage': ['ì¸í„°ë„· ì°¨ë‹¨', 'í†µì‹  ì¥ì• ', 'ì •ì „ ì‚¬íƒœ', 'ì„œë¹„ìŠ¤ ì¥ì• ', 'ì ‘ì† ì¥ì• ',
               'internet shutdown', 'network outage', 'power outage', 'service down'],
    'holiday': ['êµ­ê²½ì¼', 'ê³µíœ´ì¼ ì§€ì •', 'ì—°íœ´ ì‹œì‘', 'ëª…ì ˆ ì—°íœ´', 'íœ´ì¼ í™•ì •',
                'national holiday', 'public holiday announced', 'holiday begins']
}

def is_relevant_news(title: str, description: str) -> tuple:
    """
    ë‰´ìŠ¤ì˜ ê´€ë ¨ì„±ì„ íŒë‹¨í•˜ê³  ì¹´í…Œê³ ë¦¬ë¥¼ ë°˜í™˜
    Returns: (is_relevant, news_type, category, priority)
    """
    text = f"{title} {description}".lower()
    
    # 1. ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œ ì²´í¬ - ê´‘ê³ /ë§ˆì¼€íŒ… ì œì™¸
    for neg_kw in NEGATIVE_KEYWORDS:
        if neg_kw.lower() in text:
            return (False, None, None, None)
    
    # 2. ê²Œì„ ë‰´ìŠ¤ ì²´í¬ (ëª…í™•í•œ ê²Œì„ í‚¤ì›Œë“œ í•„ìˆ˜)
    for game_kw in GAMING_REQUIRED_KEYWORDS:
        if game_kw.lower() in text:
            return (True, 'gaming', 'gaming', 'medium')
    
    # 3. íŠ¸ë˜í”½ ì˜í–¥ ë‰´ìŠ¤ ì²´í¬ (êµ¬ì²´ì ì¸ ì´ë²¤íŠ¸ í‚¤ì›Œë“œ í•„ìˆ˜)
    for category, keywords in TRAFFIC_IMPACT_KEYWORDS.items():
        for kw in keywords:
            if kw.lower() in text:
                priority = 'high' if category in ['disaster', 'conflict', 'outage'] else 'medium'
                return (True, 'traffic_impact', category, priority)
    
    # 4. ì–´ë””ì—ë„ í•´ë‹¹ ì•ˆ ë¨ - ì œì™¸
    return (False, None, None, None)


def fetch_from_naver(keywords: List[str], max_results: int = 50) -> List[Dict]:
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ APIë¡œ êµ­ë‚´ ë‰´ìŠ¤ ê²€ìƒ‰ (ì—„ê²©í•œ í•„í„°ë§)
    - ì¼ 25,000íšŒ ë¬´ë£Œ
    - êµ­ë‚´ 300+ ì–¸ë¡ ì‚¬ ì»¤ë²„
    - ê´€ë ¨ ì—†ëŠ” ë‰´ìŠ¤ ìë™ ì œì™¸
    
    API ë°œê¸‰: https://developers.naver.com/
    """
    client_id = os.getenv('NAVER_CLIENT_ID')
    client_secret = os.getenv('NAVER_CLIENT_SECRET')
    
    if not client_id or not client_secret:
        logger.info("NAVER API í‚¤ ì—†ìŒ - ë„¤ì´ë²„ ìŠ¤í‚µ")
        return []
    
    try:
        import requests
        import urllib.parse
        
        results = []
        filtered_count = 0
        
        for keyword in keywords[:15]:  # ìµœëŒ€ 15ê°œ í‚¤ì›Œë“œ
            try:
                response = requests.get(
                    f"https://openapi.naver.com/v1/search/news.json",
                    params={
                        "query": keyword,
                        "display": 10,  # ìµœëŒ€ 10ê°œ
                        "sort": "date"  # ìµœì‹ ìˆœ
                    },
                    headers={
                        "X-Naver-Client-Id": client_id,
                        "X-Naver-Client-Secret": client_secret
                    },
                    timeout=10
                )
                
                if response.status_code == 200:
                    data = response.json()
                    items = data.get('items', [])
                    
                    for item in items:
                        # HTML íƒœê·¸ ì œê±°
                        title = item.get('title', '').replace('<b>', '').replace('</b>', '')
                        description = item.get('description', '').replace('<b>', '').replace('</b>', '')
                        
                        # ê´€ë ¨ì„± ê²€ì‚¬ (ì—„ê²©í•œ í•„í„°ë§)
                        is_relevant, news_type, category, priority = is_relevant_news(title, description)
                        
                        if not is_relevant:
                            filtered_count += 1
                            continue  # ê´€ë ¨ ì—†ëŠ” ë‰´ìŠ¤ ì œì™¸
                        
                        # ë‚ ì§œ íŒŒì‹± (RFC 2822 í˜•ì‹)
                        pub_date = item.get('pubDate', '')
                        try:
                            from email.utils import parsedate_to_datetime
                            dt = parsedate_to_datetime(pub_date)
                            date_str = dt.strftime('%Y-%m-%d')
                        except:
                            date_str = datetime.now().strftime('%Y-%m-%d')
                        
                        news_item = {
                            'date': date_str,
                            'country': 'Korea',
                            'continent': 'ASIA',
                            'title': title,
                            'summary': description[:500],
                            'url': item.get('originallink') or item.get('link', ''),
                            'source': 'ë„¤ì´ë²„ ë‰´ìŠ¤',
                            'category': category,
                            'news_type': news_type,
                            'priority': priority,
                            'api_source': 'naver'
                        }
                        
                        results.append(news_item)
                    
                    logger.info(f"ë„¤ì´ë²„ '{keyword}': {len(items)}ê°œ ì¤‘ ê´€ë ¨ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘")
                else:
                    logger.warning(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {response.status_code} - {response.text[:100]}")
                
                time.sleep(0.1)  # Rate limit (ì´ˆë‹¹ 10íšŒ ì œí•œ)
                
            except Exception as e:
                logger.error(f"ë„¤ì´ë²„ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        seen_urls = set()
        unique_results = []
        for item in results:
            if item['url'] not in seen_urls:
                seen_urls.add(item['url'])
                unique_results.append(item)
        
        logger.info(f"ë„¤ì´ë²„ ì´ ìˆ˜ì§‘: {len(unique_results)}ê°œ (í•„í„°ë§ ì œì™¸: {filtered_count}ê°œ)")
        return unique_results[:max_results]
        
    except Exception as e:
        logger.error(f"ë„¤ì´ë²„ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


# ============================================================
# DeepSearch News API (êµ­ë‚´/í•´ì™¸ ê³ í’ˆì§ˆ ë‰´ìŠ¤)
# ============================================================

def fetch_from_deepsearch(keywords: List[str], countries: List[str] = None, max_results: int = 50) -> List[Dict]:
    """
    DeepSearch News APIë¡œ ê³ í’ˆì§ˆ ë‰´ìŠ¤ ê²€ìƒ‰
    - êµ­ë‚´: ì¡°ì„ , í•œê²¨ë ˆ, ë™ì•„ ë“±
    - í•´ì™¸: NYT, BBC, Washington Post, CNN ë“±
    
    API ë¬¸ì„œ: https://api-v2.deepsearch.com
    """
    api_key = os.getenv('DEEPSEARCH_API_KEY')
    if not api_key:
        logger.info("DEEPSEARCH_API_KEY ì—†ìŒ - DeepSearch ìŠ¤í‚µ")
        return []
    
    try:
        import requests
        from datetime import datetime, timedelta
        
        results = []
        
        # ë‚ ì§œ ë²”ìœ„ (ìµœê·¼ 24ì‹œê°„)
        date_to = datetime.now().strftime('%Y-%m-%d')
        date_from = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        for keyword in keywords[:10]:  # ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œ
            try:
                # í•´ì™¸ ë‰´ìŠ¤ ê²€ìƒ‰ (global-articles)
                response = requests.get(
                    "https://api-v2.deepsearch.com/v1/global-articles",
                    params={
                        "api_key": api_key,
                        "keyword": keyword,
                        "date_from": date_from,
                        "date_to": date_to,
                        "page_size": 10,
                        "page": 1
                    },
                    timeout=15
                )
                
                if response.status_code == 200:
                    data = response.json()
                    articles = data.get('data', [])
                    
                    for article in articles:
                        news_item = {
                            'date': article.get('published_at', '')[:10] if article.get('published_at') else date_to,
                            'country': None,
                            'continent': None,
                            'title': article.get('title', ''),
                            'summary': article.get('summary', '')[:500],
                            'url': article.get('url', ''),
                            'source': article.get('publisher', 'DeepSearch'),
                            'category': 'other',
                            'news_type': 'traffic_impact',
                            'priority': 'high',
                            'api_source': 'deepsearch'
                        }
                        
                        # êµ­ê°€ ì¶”ë¡  (í‚¤ì›Œë“œì—ì„œ)
                        if countries:
                            for country in countries:
                                if country.lower() in keyword.lower() or country.lower() in news_item['title'].lower():
                                    news_item['country'] = country
                                    news_item['continent'] = get_continent(country)
                                    break
                        
                        results.append(news_item)
                    
                    logger.info(f"DeepSearch '{keyword}': {len(articles)}ê°œ ìˆ˜ì§‘")
                else:
                    logger.warning(f"DeepSearch API ì˜¤ë¥˜: {response.status_code}")
                
                time.sleep(0.5)  # Rate limit
                
            except Exception as e:
                logger.error(f"DeepSearch í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_results = []
        for item in results:
            if item['url'] not in seen_urls:
                seen_urls.add(item['url'])
                unique_results.append(item)
        
        logger.info(f"DeepSearch ì´ ìˆ˜ì§‘: {len(unique_results)}ê°œ (ì¤‘ë³µ ì œê±°ë¨)")
        return unique_results[:max_results]
        
    except Exception as e:
        logger.error(f"DeepSearch API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


def fetch_trending_from_deepsearch(sections: List[str] = None) -> List[Dict]:
    """
    DeepSearchì—ì„œ íŠ¸ë Œë”© í† í”½ ê°€ì ¸ì˜¤ê¸°
    - í•´ì™¸ ì£¼ìš” ì´ìŠˆ ìë™ ìˆ˜ì§‘
    """
    api_key = os.getenv('DEEPSEARCH_API_KEY')
    if not api_key:
        return []
    
    try:
        import requests
        
        sections = sections or ['world', 'business', 'technology']
        results = []
        
        for section in sections:
            try:
                response = requests.get(
                    f"https://api-v2.deepsearch.com/v1/global-articles/topics/{section}/trending",
                    params={
                        "api_key": api_key,
                        "page_size": 5
                    },
                    timeout=15
                )
                
                if response.status_code == 200:
                    data = response.json()
                    topics = data.get('data', [])
                    
                    for topic in topics:
                        news_item = {
                            'date': topic.get('date', '')[:10] if topic.get('date') else '',
                            'country': None,
                            'continent': None,
                            'title': topic.get('title', '') or topic.get('title_kr', ''),
                            'summary': topic.get('briefing', '')[:500],
                            'url': f"https://deepsearch.com/topic/{topic.get('id', '')}",
                            'source': 'DeepSearch Trending',
                            'category': 'major_event',
                            'news_type': 'traffic_impact',
                            'priority': 'high',
                            'api_source': 'deepsearch_trending'
                        }
                        results.append(news_item)
                    
                    logger.info(f"DeepSearch Trending '{section}': {len(topics)}ê°œ")
                    
            except Exception as e:
                logger.error(f"DeepSearch Trending '{section}' ì‹¤íŒ¨: {e}")
                continue
            
            time.sleep(0.3)
        
        return results
        
    except Exception as e:
        logger.error(f"DeepSearch Trending ì‹¤íŒ¨: {e}")
        return []


# ============================================================
# ë‹¨ê³„ë³„ AI ì •ì œ (ìµœì í™”)
# ============================================================

def smart_refine_batch(news_items: List[Dict], use_paid_api: bool = False) -> List[Dict]:
    """
    ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ ì •ì œ
    1ë‹¨ê³„: Groq (ë¬´ë£Œ, ë¹ ë¦„) - ê¸°ë³¸ ë¶„ë¥˜ + ìš”ì•½
    2ë‹¨ê³„: OpenAI/Claude (ìœ ë£Œ, ì„ íƒì ) - ìƒìœ„ 10ê°œ ìµœì¢… ê²€ì¦
    """
    if not news_items:
        return []
    
    logger.info(f"ìŠ¤ë§ˆíŠ¸ ì •ì œ ì‹œì‘: {len(news_items)}ê°œ")
    
    # ìºì‹œ ë¡œë“œ
    cache = load_cache()
    cached_count = 0
    to_process = []
    
    for item in news_items:
        cache_key = get_cache_key(item.get('title', '') + item.get('url', ''))
        if cache_key in cache:
            # ìºì‹œì—ì„œ ê²°ê³¼ ë³µì›
            cached = cache[cache_key]
            item.update(cached)
            cached_count += 1
        else:
            to_process.append(item)
    
    if cached_count > 0:
        logger.info(f"ìºì‹œì—ì„œ {cached_count}ê°œ ë³µì›")
    
    if not to_process:
        return news_items
    
    # 1ë‹¨ê³„: Groqìœ¼ë¡œ ë¹ ë¥¸ ë¶„ë¥˜ (ë¬´ë£Œ, ì´ˆê³ ì†)
    groq_key = os.getenv('GROQ_API_KEY')
    if groq_key:
        logger.info("1ë‹¨ê³„: Groq (Llama 3.1)ìœ¼ë¡œ ë¹ ë¥¸ ë¶„ë¥˜...")
        to_process = fetch_from_groq(to_process)
    
    # 2ë‹¨ê³„: ìœ ë£Œ API (ì„ íƒì , ìƒìœ„ 10ê°œë§Œ)
    if use_paid_api:
        openai_key = os.getenv('OPENAI_API_KEY')
        claude_key = os.getenv('CLAUDE_API_KEY') or os.getenv('ANTHROPIC_API_KEY')
        
        if openai_key or claude_key:
            # HIGH priority ì¤‘ ìƒìœ„ 10ê°œë§Œ ìœ ë£Œ APIë¡œ ê²€ì¦
            high_priority = [n for n in to_process if n.get('priority') == 'high']
            top_news = high_priority[:10]
            
            if top_news:
                logger.info(f"2ë‹¨ê³„: ìœ ë£Œ APIë¡œ ì‹¬ì¸µ ë¶„ì„ ({len(top_news)}ê°œ)...")
                
                for item in top_news:
                    api_type = 'openai' if openai_key else 'claude'
                    refined = refine_news_with_ai(item, api_type)
                    if refined:
                        item.update(refined)
                        item['api_source'] = api_type
                    time.sleep(0.5)
    
    # ìºì‹œ ì—…ë°ì´íŠ¸
    for item in to_process:
        cache_key = get_cache_key(item.get('title', '') + item.get('url', ''))
        cache[cache_key] = {
            'category': item.get('category'),
            'category_group': item.get('category_group'),
            'traffic_impact': item.get('traffic_impact'),
            'api_source': item.get('api_source')
        }
    
    save_cache(cache)
    
    # category_group ë§¤í•‘
    for item in news_items:
        if not item.get('category_group'):
            item['category_group'] = map_to_group_category(item.get('category', 'other'))
    
    logger.info(f"ìŠ¤ë§ˆíŠ¸ ì •ì œ ì™„ë£Œ: {len(news_items)}ê°œ")
    return news_items


# ============================================================
# ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ (HIGH/MEDIUM/LOW Priority)
# ============================================================

# HIGH Priority í‚¤ì›Œë“œ (AI ì •ì œ í•„ìˆ˜ - íŠ¸ë˜í”½ ì˜í–¥ ë‰´ìŠ¤)
HIGH_PRIORITY_KEYWORDS = {
    'critical': [
        'internet shutdown', 'blackout', 'power outage', 'outage',
        'war', 'explosion', 'bombing', 'attack', 'terrorism', 'terrorist',
        'earthquake', 'flood', 'disaster', 'emergency', 'tsunami', 'typhoon',
        'curfew', 'protest', 'riot', 'strike', 'unrest',
        'shutdown', 'ban', 'block', 'censorship'
    ],
    'countries': [
        'Iraq', 'Pakistan', 'Turkey', 'Russia', 'Egypt',
        'Saudi Arabia', 'Indonesia', 'Hong Kong', 'Iran', 'Syria',
        'Baghdad', 'Karachi', 'Istanbul', 'Moscow', 'Cairo', 'Jakarta'
    ]
}

# MEDIUM Priority í‚¤ì›Œë“œ (ê·œì¹™ ê¸°ë°˜ ìë™ ë¶„ë¥˜) - ë” ì—„ê²©í•œ ê²Œì„ í‚¤ì›Œë“œ
MEDIUM_RULES = {
    'gaming': [
        # PUBG/Krafton ê´€ë ¨ (í•„ìˆ˜)
        'PUBG', 'pubg mobile', 'battlegrounds mobile', 'Krafton', 'BGMI',
        'PMGC', 'PMPL', 'pubg esports',
        # ê²½ìŸì‘
        'Free Fire', 'Call of Duty Mobile', 'COD Mobile',
        # ê²Œì„ ì—…ê³„ (êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë§Œ)
        'mobile game revenue', 'mobile game update', 'game patch',
        'esports tournament', 'e-sports championship',
        # í•œêµ­ì–´
        'íì§€', 'ë°°í‹€ê·¸ë¼ìš´ë“œ', 'í¬ë˜í”„í†¤', 'ëª¨ë°”ì¼ê²Œì„ ë§¤ì¶œ', 'ê²Œì„ ì—…ë°ì´íŠ¸'
    ],
    'holiday': [
        # ì‹¤ì œ ê³µíœ´ì¼ë§Œ (ë§ˆì¼€íŒ… ì œì™¸)
        'national holiday', 'public holiday', 'bank holiday',
        'Eid al-Fitr', 'Eid al-Adha', 'Christmas Day', 'New Year Day',
        'Ramadan begins', 'Diwali celebration',
        'êµ­ê²½ì¼', 'ê³µíœ´ì¼', 'ëª…ì ˆ ì—°íœ´', 'ì¶”ì„', 'ì„¤ë‚ '
    ],
    'school': [
        # í•™ì‚¬ì¼ì •
        'school holiday', 'school vacation', 'exam period', 'semester break',
        'summer vacation', 'winter vacation',
        'ë°©í•™ ì‹œì‘', 'ì‹œí—˜ ê¸°ê°„', 'ê°œí•™'
    ]
}

# LOW Priority (ì œì™¸í•  íŒ¨í„´) - í™•ì¥ëœ ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œ
# â€» protest/ì‹œìœ„ëŠ” íŠ¸ë˜í”½ ì˜í–¥ ìˆìœ¼ë¯€ë¡œ ì œì™¸í•˜ì§€ ì•ŠìŒ!
# â€» KTëŠ” í†µì‹ ì¥ì•  ë‰´ìŠ¤ì— í•„ìš”í•˜ë¯€ë¡œ 'KT ìœ„ì¦ˆ'ë¡œ êµ¬ì²´í™”!
# â€» ì›”ë“œì»µ/ì˜¬ë¦¼í”½ì€ íŠ¸ë˜í”½ ì˜í–¥ ìˆìœ¼ë¯€ë¡œ ì œì™¸í•˜ì§€ ì•ŠìŒ!
EXCLUDE_PATTERNS = [
    # ========== ê´‘ê³ /ë§ˆì¼€íŒ… (ê°•í™”) ==========
    'ê´‘ê³ ', 'sponsored', 'affiliate', 'promotion', 'í”„ë¡œëª¨ì…˜',
    'ìº í˜ì¸', 'campaign', 'íŒì—…', 'popup', 'ì½œë¼ë³´', 'collaboration',
    'ì¶œì‹œ', 'launch', 'ì‹ ì œí’ˆ', 'í• ì¸', 'sale', 'ì„¸ì¼',
    'í˜‘ì°¬', 'ë§ˆì¼€íŒ…', 'PPL', 'ë³´ë„ìë£Œ', 'branded content',
    
    # ========== ê¸ˆìœµ/ì¦ì‹œ/íˆ¬ì (ê°•í™”) ==========
    'ì£¼ì‹', 'ì¦ì‹œ', 'ì½”ìŠ¤í”¼', 'ì½”ìŠ¤ë‹¥', 'ë‚˜ìŠ¤ë‹¥',
    'ì¥ì¤‘', 'ì¥ ë§ˆê°', 'ì¥ ì´ˆë°˜', 'ë§ˆê° ì§€ìˆ˜',
    'ì£¼ê°€', 'ì£¼ì‹ì‹œì¥', 'íˆ¬ìì', 'ê¸°ê´€íˆ¬ìì',
    'ì¦ê¶Œì‚¬', 'ì¦ê¶Œê°€', 'ë¦¬í¬íŠ¸', 'ë¦¬ì„œì¹˜ì„¼í„°',
    'ì‹¤ì ë°œí‘œ', 'ë¶„ê¸° ì‹¤ì ', 'ì—°ê°„ ì‹¤ì ',
    'stock price', 'earnings', 'quarterly earnings', 'annual earnings',
    'investor', 'íˆ¬ì', 'ë°°ë‹¹', 'dividend',
    'í€ë“œ', 'ETF', 'ë¦¬ì¸ ', 'ì¬í…Œí¬', 'IPO', 'ê³µëª¨ì£¼',
    'IR', 'conference call',
    
    # ========== ì±„ìš©/ì»¤ë¦¬ì–´ (ê°•í™”) ==========
    'ì±„ìš©', 'ê³µì±„', 'ìˆ˜ì‹œì±„ìš©', 'ì±„ìš© ê³µê³ ',
    'ì‹ ì…ì‚¬ì›', 'ê²½ë ¥ì§', 'ì¸ì¬ ì±„ìš©',
    'êµ¬ì¸', 'êµ¬ì¸ ê³µê³ ',
    'hiring', 'job opening', 'career', 'recruitment',
    'career fair', 'ì±„ìš© ì„¤ëª…íšŒ', 'ì¸í„´ ëª¨ì§‘', 'ê³µëª¨ì „',
    
    # ========== ì—°ì˜ˆ/ì—”í„°í…Œì¸ë¨¼íŠ¸ (ê°•í™”) ==========
    'ê±¸ê·¸ë£¹', 'ë³´ì´ê·¸ë£¹', 'ì•„ì´ëŒ', 'idol', 'K-pop',
    'ì½˜ì„œíŠ¸', 'concert', 'ì•¨ë²”', 'album', 'ë®¤ì§ë¹„ë””ì˜¤', 'íŒ¬ë¯¸íŒ…', 'fan meeting',
    'MAMA', 'Awards', 'ì‹œìƒì‹', 'mourning',
    
    # ========== ì—°ì˜ˆ/OTT/ê°€ì‹­ (ì¶”ê°€) ==========
    'ë“œë¼ë§ˆ', 'ì˜ˆëŠ¥', 'ì‹œì²­ë¥ ', 'ì˜ˆëŠ¥ í”„ë¡œê·¸ë¨', 'ë¦¬ì–¼ë¦¬í‹°ì‡¼',
    'OTT', 'ë„·í”Œë¦­ìŠ¤', 'ë””ì¦ˆë‹ˆ+', 'í‹°ë¹™', 'ì›¨ì´ë¸Œ', 'ì¿ íŒ¡í”Œë ˆì´',
    'Netflix', 'Disney+', 'OST',
    'ì—´ì• ì„¤', 'ê²°ë³„ì„¤', 'ì—°ì˜ˆê³„', 'ì—°ì˜ˆì¸ ì»¤í”Œ', 'ìŠ¤ìº”ë“¤',
    'ì˜í™”ì œ', 'ë ˆë“œì¹´í«', 'celebrity', 'entertainment news', 'showbiz',
    
    # ========== ìŒì‹/ë¸Œëœë“œ ==========
    'ë˜í‚¨', 'ìŠ¤íƒ€ë²…ìŠ¤', 'ë§¥ë„ë‚ ë“œ', 'ë²„ê±°í‚¹', 'ì— ì•¤ì— ', 'M&M',
    'ì´ˆì½œë¦¿', 'ì»¤í”¼', 'coffee', 'ìŒë£Œ',
    
    # ========== íŒ¨ì…˜/ë·°í‹° ==========
    'íŒ¨ì…˜', 'fashion', 'ë·°í‹°', 'beauty', 'í™”ì¥í’ˆ', 'cosmetic',
    'ì˜ë¥˜', 'clothing', 'ì‡¼í•‘', 'shopping',
    
    # ========== êµ°ì‚¬/ë°©ì‚° ==========
    'ìì£¼í¬', 'ì „ì°¨', 'ë¯¸ì‚¬ì¼', 'ë¬´ê¸°', 'êµ°ìˆ˜', 'ë°©ì‚°', 'êµ­ë°©',
    'K9', 'K2', 'í•œí™”ì—ì–´ë¡œ', 'í•œí™”ë””íœìŠ¤', 'defense contract',
    'military contract', 'arms deal', 'ë°©ìœ„ì‚¬ì—…',
    'DMZ', 'Korean War soldiers', 'ìœ í•´ ë°œêµ´', 'ì „ì‚¬ì',
    
    # ========== ì •ì¹˜/ì™¸êµ (ì¼ë°˜) ==========
    'ëŒ€í†µë ¹', 'êµ­íšŒ', 'ì™¸êµë¶€', 'ì¥ê´€', 'ì •ìƒíšŒë‹´', 'summit',
    'ì¡°ì•½', 'treaty', 'í˜‘ì •',
    
    # ========== ìŠ¤í¬ì¸  (eìŠ¤í¬ì¸ /ì›”ë“œì»µ/ì˜¬ë¦¼í”½ ì œì™¸) ==========
    # â€» KT ìœ„ì¦ˆë§Œ ì œì™¸ (KT í†µì‹ ì¥ì• ëŠ” ì‚´ë ¤ì•¼ í•¨!)
    'KT ìœ„ì¦ˆ', 'kt wiz', 'KTìœ„ì¦ˆ',
    'í”„ë¡œì¶•êµ¬', 'í”„ë¡œì•¼êµ¬', 'NBA', 'MLB',
    'ì•¼êµ¬ ê²°ê³¼', 'ì¶•êµ¬ ê²°ê³¼', 'ê²½ê¸° ê²°ê³¼',
    'êµ¬ì›íˆ¬ìˆ˜', 'ìŠ¤í† ë¸Œë¦¬ê·¸', 'WAR ì „ì²´',
    # â€» FIFA/ì›”ë“œì»µ/ì˜¬ë¦¼í”½ì€ íŠ¸ë˜í”½ ì˜í–¥ ìˆìœ¼ë¯€ë¡œ ì œì™¸í•˜ì§€ ì•ŠìŒ!
    
    # ========== ë¶€ë™ì‚°/ì£¼ê±° (ì¶”ê°€) ==========
    'ë¶„ì–‘', 'ì²­ì•½', 'ì…ì£¼ì ëª¨ì§‘',
    'ì „ì„¸', 'ì›”ì„¸', 'ë§¤ë§¤', 'ì „ì›”ì„¸',
    'ì „ì„¸ê°€', 'ë§¤ë§¤ê°€', 'ì§‘ê°’', 'ì•„íŒŒíŠ¸ ë‹¨ì§€',
    'ì˜¤í”¼ìŠ¤í…”', 'ìƒê°€ ë¶„ì–‘', 'ì˜¤í”¼ìŠ¤ ì„ëŒ€',
    'ë¶€ë™ì‚° ì‹œì¥', 'ë¶€ë™ì‚° ê·œì œ',
    'ë¶€ë™ì‚°', 'real estate', 'housing market',
    
    # ========== ë‚ ì”¨/ìƒí™œì •ë³´ (ì¶”ê°€) ==========
    # â€» ëŒ€í˜• ìì—°ì¬í•´ëŠ” íŠ¸ë˜í”½ ì˜í–¥ìœ¼ë¡œ ì¡ìœ¼ë¯€ë¡œ ì¼ìƒ ë‚ ì”¨ë§Œ ì œì™¸
    'ì˜¤ëŠ˜ì˜ ë‚ ì”¨', 'ì£¼ê°„ ë‚ ì”¨', 'ê¸°ìƒì²­',
    'ê¸°ì˜¨', 'ë¯¸ì„¸ë¨¼ì§€', 'ì²´ê°ì˜¨ë„',
    'ë‚ ì”¨', 'weather forecast', 'weekly forecast',
    
    # ========== ë ˆì‹œí”¼/ë§›ì§‘/ìƒí™œ (ì¶”ê°€) ==========
    'ë ˆì‹œí”¼', 'ìš”ë¦¬ë²•', 'ì§‘ë°¥', 'ê°„í¸ì‹',
    'ë§›ì§‘ íƒë°©', 'ì‹ë‹¹ ë¦¬ë·°', 'ì¹´í˜ ì¶”ì²œ',
    'restaurant review', 'food blog',
    'ë§›ì§‘', 'restaurant', 'ì—¬í–‰', 'travel tip',
    
    # ========== ì‹œìœ„ - êµ¬ì²´ì  ì¼€ì´ìŠ¤ë§Œ ==========
    # â€» ì¼ë°˜ protest/ì‹œìœ„ëŠ” íŠ¸ë˜í”½ ì˜í–¥ ìˆìœ¼ë¯€ë¡œ ì œì™¸í•˜ì§€ ì•ŠìŒ!
    'immigration protest', 'hindu protest', 'farmer protest',
]


def classify_news_priority(title: str, summary: str) -> tuple:
    """
    ë‰´ìŠ¤ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ë¶„ë¥˜ (ì—„ê²©í•œ ê¸°ì¤€)
    
    Args:
        title: ë‰´ìŠ¤ ì œëª©
        summary: ë‰´ìŠ¤ ìš”ì•½
    
    Returns:
        (priority, news_type, auto_category)
        - priority: 'high', 'medium', 'low'
        - news_type: 'traffic_impact', 'gaming', None
        - auto_category: ìë™ ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬ (mediumì¸ ê²½ìš°)
    """
    text = f"{title} {summary}".lower()
    
    # 1. LOW Priority ì²´í¬ (ì œì™¸) - ê´‘ê³ /ë§ˆì¼€íŒ…/ë¹„ê´€ë ¨ ë‰´ìŠ¤
    for pattern in EXCLUDE_PATTERNS:
        if pattern.lower() in text:
            return ('low', None, None)
    
    # 2. HIGH Priority ì²´í¬ (íŠ¸ë˜í”½ ì˜í–¥ ë‰´ìŠ¤ - AI ì •ì œ ëŒ€ìƒ)
    for keyword in HIGH_PRIORITY_KEYWORDS['critical']:
        if keyword.lower() in text:
            return ('high', 'traffic_impact', None)
    
    for country in HIGH_PRIORITY_KEYWORDS['countries']:
        if country.lower() in text:
            # êµ­ê°€ ì–¸ê¸‰ + ìœ„ê¸° í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ HIGH
            for keyword in HIGH_PRIORITY_KEYWORDS['critical']:
                if keyword.lower() in text:
                    return ('high', 'traffic_impact', None)
    
    # 3. MEDIUM Priority ì²´í¬ (ê·œì¹™ ê¸°ë°˜ ìë™ ë¶„ë¥˜)
    for category, keywords in MEDIUM_RULES.items():
        for keyword in keywords:
            if keyword.lower() in text:
                if category == 'gaming':
                    return ('medium', 'gaming', 'gaming')
                elif category == 'holiday':
                    return ('medium', 'traffic_impact', 'holiday')
                elif category == 'school':
                    return ('medium', 'traffic_impact', 'school_calendar')
    
    # 4. ê¸°ë³¸ê°’: LOW (ê´€ë ¨ ì—†ìœ¼ë©´ ì œì™¸!)
    # ì´ì „: ('medium', 'gaming', 'gaming') - ëª¨ë“  ë‰´ìŠ¤ê°€ ê²Œì„ìœ¼ë¡œ ë¶„ë¥˜ë¨
    # ìˆ˜ì •: ('low', None, None) - ê´€ë ¨ ì—†ëŠ” ë‰´ìŠ¤ëŠ” ì œì™¸
    return ('low', None, None)


def clean_html_tags(text: str) -> str:
    """HTML íƒœê·¸ ì œê±°"""
    import re
    # HTML íƒœê·¸ ì œê±°
    clean = re.sub(r'<[^>]+>', '', text)
    # HTML ì—”í‹°í‹° ì œê±°
    clean = re.sub(r'&[a-zA-Z]+;', ' ', clean)
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    clean = re.sub(r'\s+', ' ', clean)
    return clean.strip()


def fetch_news_from_openai(keyword: str, countries: List[Dict] = None) -> List[Dict]:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        countries: ê´€ë ¨ êµ­ê°€ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        logger.warning("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    try:
        import requests
        
        # êµ­ê°€ ì •ë³´ í¬í•¨
        country_context = ""
        if countries:
            country_names = [c.get('country', '') for c in countries if c.get('country')]
            if country_names:
                country_context = f" íŠ¹íˆ {', '.join(country_names[:5])} êµ­ê°€ì™€ ê´€ë ¨ëœ"
        
        prompt = f"""ë‹¤ìŒ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”: {keyword}{country_context}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ìµœëŒ€ 10ê°œ):
[
  {{
    "title": "ë‰´ìŠ¤ ì œëª©",
    "summary": "ìš”ì•½ (2-3ë¬¸ì¥)",
    "url": "ë‰´ìŠ¤ ë§í¬ (ê°€ëŠ¥í•œ ê²½ìš°)",
    "source": "ì¶œì²˜",
    "date": "YYYY-MM-DD í˜•ì‹",
    "country": "ê´€ë ¨ êµ­ê°€ (ì—†ìœ¼ë©´ null)",
    "reason": "íŠ¸ë˜í”½ ë³€í™”ì™€ì˜ ì—°ê´€ì„± ë¶„ì„"
  }}
]

ìµœê·¼ 7ì¼ ì´ë‚´ì˜ ë‰´ìŠ¤ë§Œ í¬í•¨í•˜ê³ , PUBG Mobileì´ë‚˜ ëª¨ë°”ì¼ ê²Œì„ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ë§Œ ì•Œë ¤ì£¼ì„¸ìš”."""

        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "gpt-4o-mini",  # ë˜ëŠ” "gpt-4", "gpt-3.5-turbo"
                "messages": [
                    {"role": "system", "content": "You are a news analyst. Return only valid JSON array."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 2000
            },
            timeout=30
        )
        
        if response.status_code != 200:
            logger.error(f"OpenAI API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return []
        
        data = response.json()
        content = data['choices'][0]['message']['content']
        
        # JSON ì¶”ì¶œ
        import re
        json_match = re.search(r'\[[\s\S]*\]', content)
        if json_match:
            news_data = json.loads(json_match.group())
            
            # í˜•ì‹ ë³€í™˜
            news_list = []
            for item in news_data:
                news_list.append({
                    'date': item.get('date', datetime.now().strftime('%Y-%m-%d')),
                    'country': item.get('country'),
                    'continent': get_continent(item.get('country', '')) if item.get('country') else None,
                    'title': item.get('title', ''),
                    'summary': item.get('summary', '')[:500],
                    'url': item.get('url', '#'),
                    'source': item.get('source', 'OpenAI'),
                    'category': 'gaming'
                })
            
            logger.info(f"OpenAI APIë¡œ '{keyword}'ì—ì„œ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_list
        else:
            logger.warning("OpenAI ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
            
    except Exception as e:
        logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


def fetch_news_from_claude(keyword: str, countries: List[Dict] = None) -> List[Dict]:
    """
    Claude APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        countries: ê´€ë ¨ êµ­ê°€ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # CLAUDE_API_KEY ë˜ëŠ” ANTHROPIC_API_KEY ì§€ì› (ë‘˜ ë‹¤ ë™ì¼)
    api_key = os.getenv('CLAUDE_API_KEY') or os.getenv('ANTHROPIC_API_KEY')
    if not api_key:
        logger.warning("CLAUDE_API_KEY ë˜ëŠ” ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    try:
        import requests
        
        # êµ­ê°€ ì •ë³´ í¬í•¨
        country_context = ""
        if countries:
            country_names = [c.get('country', '') for c in countries if c.get('country')]
            if country_names:
                country_context = f" íŠ¹íˆ {', '.join(country_names[:5])} êµ­ê°€ì™€ ê´€ë ¨ëœ"
        
        prompt = f"""ë‹¤ìŒ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”: {keyword}{country_context}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ìµœëŒ€ 10ê°œ):
[
  {{
    "title": "ë‰´ìŠ¤ ì œëª©",
    "summary": "ìš”ì•½ (2-3ë¬¸ì¥)",
    "url": "ë‰´ìŠ¤ ë§í¬ (ê°€ëŠ¥í•œ ê²½ìš°)",
    "source": "ì¶œì²˜",
    "date": "YYYY-MM-DD í˜•ì‹",
    "country": "ê´€ë ¨ êµ­ê°€ (ì—†ìœ¼ë©´ null)",
    "reason": "íŠ¸ë˜í”½ ë³€í™”ì™€ì˜ ì—°ê´€ì„± ë¶„ì„"
  }}
]

ìµœê·¼ 7ì¼ ì´ë‚´ì˜ ë‰´ìŠ¤ë§Œ í¬í•¨í•˜ê³ , PUBG Mobileì´ë‚˜ ëª¨ë°”ì¼ ê²Œì„ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ë§Œ ì•Œë ¤ì£¼ì„¸ìš”."""

        response = requests.post(
            "https://api.anthropic.com/v1/messages",
            headers={
                "x-api-key": api_key,
                "anthropic-version": "2023-06-01",
                "Content-Type": "application/json"
            },
            json={
                "model": "claude-3-5-sonnet-20241022",  # ë˜ëŠ” "claude-3-opus-20240229"
                "max_tokens": 2000,
                "messages": [
                    {"role": "user", "content": prompt}
                ]
            },
            timeout=30
        )
        
        if response.status_code != 200:
            logger.error(f"Claude API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return []
        
        data = response.json()
        content = data['content'][0]['text']
        
        # JSON ì¶”ì¶œ
        import re
        json_match = re.search(r'\[[\s\S]*\]', content)
        if json_match:
            news_data = json.loads(json_match.group())
            
            # í˜•ì‹ ë³€í™˜
            news_list = []
            for item in news_data:
                news_list.append({
                    'date': item.get('date', datetime.now().strftime('%Y-%m-%d')),
                    'country': item.get('country'),
                    'continent': get_continent(item.get('country', '')) if item.get('country') else None,
                    'title': item.get('title', ''),
                    'summary': item.get('summary', '')[:500],
                    'url': item.get('url', '#'),
                    'source': item.get('source', 'Claude'),
                    'category': 'gaming'
                })
            
            logger.info(f"Claude APIë¡œ '{keyword}'ì—ì„œ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_list
        else:
            logger.warning("Claude ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
            
    except Exception as e:
        logger.error(f"Claude API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


def fetch_news_from_api(keyword: str, api_type: str = 'rss', countries: List[Dict] = None) -> List[Dict]:
    """
    APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (í™•ì¥ ê°€ëŠ¥)
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        api_type: API íƒ€ì… ('rss', 'openai', 'claude', 'gemini' ë“±)
        countries: ê´€ë ¨ êµ­ê°€ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # RSSëŠ” ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
    if api_type == 'rss':
        return fetch_news_from_rss(keyword)
    
    # OpenAI API ì‚¬ìš©
    elif api_type == 'openai':
        news = fetch_news_from_openai(keyword, countries)
        if news:
            return news
        else:
            logger.info("OpenAI API ì‹¤íŒ¨, RSSë¡œ í´ë°±")
            return fetch_news_from_rss(keyword)
    
    # Claude API ì‚¬ìš©
    elif api_type == 'claude':
        news = fetch_news_from_claude(keyword, countries)
        if news:
            return news
        else:
            logger.info("Claude API ì‹¤íŒ¨, RSSë¡œ í´ë°±")
            return fetch_news_from_rss(keyword)
    
    # Gemini API ì‚¬ìš© (API í‚¤ í•„ìš”)
    elif api_type == 'gemini':
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. RSSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return fetch_news_from_rss(keyword)
        
        try:
            # Gemini API í˜¸ì¶œ ë¡œì§ (ì¶”í›„ êµ¬í˜„)
            logger.info(f"Gemini API ì‚¬ìš© (í‚¤ì›Œë“œ: {keyword})")
            return []  # TODO: Gemini API êµ¬í˜„
        except Exception as e:
            logger.error(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return fetch_news_from_rss(keyword)  # í´ë°±
    
    else:
        logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” API íƒ€ì…: {api_type}. RSSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return fetch_news_from_rss(keyword)


def fetch_news_from_rss(keyword: str, max_retries: int = 3) -> List[Dict]:
    """
    Google News RSSì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    news_list = []
    # URL ì¸ì½”ë”©ìœ¼ë¡œ íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
    import urllib.parse
    encoded_keyword = urllib.parse.quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"
    
    for attempt in range(max_retries):
        try:
            # ë¡œê·¸ì—ëŠ” í‚¤ì›Œë“œë§Œ í‘œì‹œ (URL ì „ì²´ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ)
            logger.info(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹œë„ {attempt + 1}/{max_retries}: í‚¤ì›Œë“œ='{keyword}'")
            feed = feedparser.parse(rss_url)
            
            if feed.bozo and feed.bozo_exception:
                logger.warning(f"RSS íŒŒì‹± ê²½ê³ : {feed.bozo_exception}")
            
            for entry in feed.entries[:10]:  # ìµœëŒ€ 10ê°œ
                # ë‚ ì§œ íŒŒì‹±
                try:
                    pub_date = datetime(*entry.published_parsed[:6])
                except (AttributeError, TypeError):
                    pub_date = datetime.now()
                
                # ì˜¤ëŠ˜ë¶€í„° 24ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤ë§Œ
                if pub_date < datetime.now() - timedelta(days=1):
                    continue
                
                # HTML íƒœê·¸ ì œê±°
                clean_title = clean_html_tags(entry.get('title', ''))
                clean_summary = clean_html_tags(entry.get('summary', ''))[:500]
                
                # ìš°ì„ ìˆœìœ„ ë° ë‰´ìŠ¤ íƒ€ì… ë¶„ë¥˜
                priority, news_type, auto_category = classify_news_priority(clean_title, clean_summary)
                
                # LOW priorityëŠ” ì œì™¸
                if priority == 'low':
                    continue
                
                news_item = {
                    'date': pub_date.strftime('%Y-%m-%d'),
                    'country': None,  # í‚¤ì›Œë“œì—ì„œ ì¶”ì¶œ
                    'continent': None,
                    'title': clean_title,
                    'summary': clean_summary,
                    'url': entry.get('link', ''),
                    'source': entry.get('source', {}).get('title', 'Google News'),
                    'category': auto_category if auto_category else 'gaming',
                    'news_type': news_type if news_type else 'gaming',
                    'priority': priority
                }
                
                # êµ­ê°€ëª… ì¶”ì¶œ (í‚¤ì›Œë“œì—ì„œ)
                keywords_config = load_keywords()
                priority_countries = keywords_config.get('priority_countries', {})
                for country in priority_countries.keys():
                    if country.lower() in keyword.lower():
                        news_item['country'] = country
                        news_item['continent'] = get_continent(country)
                        break
                
                news_list.append(news_item)
            
            logger.info(f"'{keyword}'ì—ì„œ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            break  # ì„±ê³µí•˜ë©´ ì¤‘ë‹¨
            
        except Exception as e:
            logger.error(f"RSS ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(5)  # 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
            else:
                logger.error(f"'{keyword}' í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
    
    return news_list


def refine_news_with_ai(news_item: Dict, api_type: str = 'openai') -> Optional[Dict]:
    """
    RSSë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ë¥¼ AIë¡œ ì •ì œ (ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜, íŠ¸ë˜í”½ ì˜í–¥ ë¶„ì„)
    
    Args:
        news_item: RSSë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ì•„ì´í…œ
        api_type: ì‚¬ìš©í•  API ('openai' ë˜ëŠ” 'claude')
    
    Returns:
        ì •ì œëœ ë‰´ìŠ¤ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None (ê´€ë ¨ ì—†ìŒ)
    """
    api_key = None
    api_url = None
    headers = {}
    payload = {}
    
    if api_type == 'openai':
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            return news_item  # API í‚¤ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        
        api_url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”:

ì œëª©: {news_item.get('title', '')}
ë‚´ìš©: {news_item.get('summary', '')}
URL: {news_item.get('url', '')}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "relevant": true ë˜ëŠ” false (ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìœ¼ë©´ true),
  "category": "ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ (ì•„ë˜ ëª©ë¡ ì°¸ê³ )",
  "country": "ê´€ë ¨ êµ­ê°€ëª… (ì—†ìœ¼ë©´ null)",
  "traffic_impact": "íŠ¸ë˜í”½ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì„¤ëª… (ê°„ë‹¨íˆ)",
  "summary_kr": "í•œêµ­ì–´ë¡œ 2-3ì¤„ ìš”ì•½"
}}

ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ì •í™•íˆ í•˜ë‚˜ ì„ íƒ):

ğŸ”´ ì¥ì•  ë° ì°¨ë‹¨ (Outage & Block):
- internet_shutdown: ì¸í„°ë„· ì°¨ë‹¨, í†µì‹  ì¥ì• 
- tech_outage: ì†Œì…œë¯¸ë””ì–´/ì•±ìŠ¤í† ì–´/í´ë¼ìš°ë“œ ì¥ì• 
- power_outage: ì •ì „, ì „ë ¥ ê³µê¸‰ ì¤‘ë‹¨
- censorship: ê²€ì—´, ì•±/ê²Œì„ ê¸ˆì§€
- cyber_attack: ì‚¬ì´ë²„ ê³µê²©, DDoS, í•´í‚¹
- infrastructure_damage: ì¸í”„ë¼ ì†ìƒ, êµëŸ‰/ê±´ë¬¼ ë¶•ê´´

ğŸŸ  ì‚¬íšŒì  ìœ„ê¸° (Social Crisis):
- war_conflict: ì „ìŸ, ë¶„ìŸ, êµ°ì‚¬ ì‘ì „
- terrorism_explosion: í…ŒëŸ¬, í­ë°œ, í­íƒ„ ê³µê²©
- natural_disaster: ì§€ì§„, í™ìˆ˜, íƒœí’, ì‚°ë¶ˆ ë“± ì²œì¬ì§€ë³€
- protest_strike: ì‹œìœ„, íŒŒì—…, í­ë™
- curfew: í†µê¸ˆ, ë´‰ì‡„, ë¹„ìƒì‚¬íƒœ
- pandemic: íŒ¬ë°ë¯¹, ì „ì—¼ë³‘, ê²©ë¦¬
- economic: ê²½ì œ ìœ„ê¸°, ì¸í”Œë ˆì´ì…˜, í†µí™” í‰ê°€ì ˆí•˜

ğŸŸ¢ ì‹œì¦Œ ë° ì¼ì • (Seasonal & Calendar):
- holiday: ê³µíœ´ì¼, ëª…ì ˆ, ì¶•ì œ
- school_calendar: ë°©í•™, ì‹œí—˜ê¸°ê°„ ë“± í•™ì‚¬ì¼ì •
- election: ì„ ê±°, íˆ¬í‘œ, ì •ì¹˜ ì´ë²¤íŠ¸

ğŸ”µ ê²Œì„ ë° ê²½ìŸ (Gaming & Competitor):
- gaming: ê²Œì„ ê´€ë ¨ ë‰´ìŠ¤
- competitor_game: ê²½ìŸ ê²Œì„ ì¶œì‹œ/ì—…ë°ì´íŠ¸
- social_trend: ë°”ì´ëŸ´ íŠ¸ë Œë“œ, ì¸í”Œë£¨ì–¸ì„œ, eìŠ¤í¬ì¸  í† ë„ˆë¨¼íŠ¸
- sports_event: ì›”ë“œì»µ, ì˜¬ë¦¼í”½ ë“± ìŠ¤í¬ì¸  ì´ë²¤íŠ¸
- major_event: ì£¼ìš” ë¬¸í™” í–‰ì‚¬, ê²Œì„ ì»¨ë²¤ì…˜

âšª ê¸°íƒ€:
- other: ë¶„ë¥˜ ë¶ˆê°€

ê´€ë ¨ì´ ì—†ìœ¼ë©´ relevant: falseë¡œ ì„¤ì •í•˜ì„¸ìš”."""

        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": "You are a news analyst. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 500
        }
    
    elif api_type == 'claude':
        api_key = os.getenv('CLAUDE_API_KEY') or os.getenv('ANTHROPIC_API_KEY')
        if not api_key:
            return news_item  # API í‚¤ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        
        api_url = "https://api.anthropic.com/v1/messages"
        headers = {
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01",
            "Content-Type": "application/json"
        }
        
        prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”:

ì œëª©: {news_item.get('title', '')}
ë‚´ìš©: {news_item.get('summary', '')}
URL: {news_item.get('url', '')}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "relevant": true ë˜ëŠ” false (ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìœ¼ë©´ true),
  "category": "ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ (ì•„ë˜ ëª©ë¡ ì°¸ê³ )",
  "country": "ê´€ë ¨ êµ­ê°€ëª… (ì—†ìœ¼ë©´ null)",
  "traffic_impact": "íŠ¸ë˜í”½ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì„¤ëª… (ê°„ë‹¨íˆ)",
  "summary_kr": "í•œêµ­ì–´ë¡œ 2-3ì¤„ ìš”ì•½"
}}

ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ì •í™•íˆ í•˜ë‚˜ ì„ íƒ):

ğŸ”´ ì¥ì•  ë° ì°¨ë‹¨ (Outage & Block):
- internet_shutdown: ì¸í„°ë„· ì°¨ë‹¨, í†µì‹  ì¥ì• 
- tech_outage: ì†Œì…œë¯¸ë””ì–´/ì•±ìŠ¤í† ì–´/í´ë¼ìš°ë“œ ì¥ì• 
- power_outage: ì •ì „, ì „ë ¥ ê³µê¸‰ ì¤‘ë‹¨
- censorship: ê²€ì—´, ì•±/ê²Œì„ ê¸ˆì§€
- cyber_attack: ì‚¬ì´ë²„ ê³µê²©, DDoS, í•´í‚¹
- infrastructure_damage: ì¸í”„ë¼ ì†ìƒ, êµëŸ‰/ê±´ë¬¼ ë¶•ê´´

ğŸŸ  ì‚¬íšŒì  ìœ„ê¸° (Social Crisis):
- war_conflict: ì „ìŸ, ë¶„ìŸ, êµ°ì‚¬ ì‘ì „
- terrorism_explosion: í…ŒëŸ¬, í­ë°œ, í­íƒ„ ê³µê²©
- natural_disaster: ì§€ì§„, í™ìˆ˜, íƒœí’, ì‚°ë¶ˆ ë“± ì²œì¬ì§€ë³€
- protest_strike: ì‹œìœ„, íŒŒì—…, í­ë™
- curfew: í†µê¸ˆ, ë´‰ì‡„, ë¹„ìƒì‚¬íƒœ
- pandemic: íŒ¬ë°ë¯¹, ì „ì—¼ë³‘, ê²©ë¦¬
- economic: ê²½ì œ ìœ„ê¸°, ì¸í”Œë ˆì´ì…˜, í†µí™” í‰ê°€ì ˆí•˜

ğŸŸ¢ ì‹œì¦Œ ë° ì¼ì • (Seasonal & Calendar):
- holiday: ê³µíœ´ì¼, ëª…ì ˆ, ì¶•ì œ
- school_calendar: ë°©í•™, ì‹œí—˜ê¸°ê°„ ë“± í•™ì‚¬ì¼ì •
- election: ì„ ê±°, íˆ¬í‘œ, ì •ì¹˜ ì´ë²¤íŠ¸

ğŸ”µ ê²Œì„ ë° ê²½ìŸ (Gaming & Competitor):
- gaming: ê²Œì„ ê´€ë ¨ ë‰´ìŠ¤
- competitor_game: ê²½ìŸ ê²Œì„ ì¶œì‹œ/ì—…ë°ì´íŠ¸
- social_trend: ë°”ì´ëŸ´ íŠ¸ë Œë“œ, ì¸í”Œë£¨ì–¸ì„œ, eìŠ¤í¬ì¸  í† ë„ˆë¨¼íŠ¸
- sports_event: ì›”ë“œì»µ, ì˜¬ë¦¼í”½ ë“± ìŠ¤í¬ì¸  ì´ë²¤íŠ¸
- major_event: ì£¼ìš” ë¬¸í™” í–‰ì‚¬, ê²Œì„ ì»¨ë²¤ì…˜

âšª ê¸°íƒ€:
- other: ë¶„ë¥˜ ë¶ˆê°€

ê´€ë ¨ì´ ì—†ìœ¼ë©´ relevant: falseë¡œ ì„¤ì •í•˜ì„¸ìš”."""

        payload = {
            "model": "claude-3-5-sonnet-20241022",
            "max_tokens": 500,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }
    
    else:
        return news_item  # ì•Œ ìˆ˜ ì—†ëŠ” API íƒ€ì…ì´ë©´ ì›ë³¸ ë°˜í™˜
    
    try:
        import requests
        response = requests.post(api_url, headers=headers, json=payload, timeout=30)
        
        if response.status_code != 200:
            logger.warning(f"AI API ì˜¤ë¥˜: {response.status_code}, ì›ë³¸ ë‰´ìŠ¤ ì‚¬ìš©")
            return news_item
        
        data = response.json()
        
        # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if api_type == 'openai':
            content = data['choices'][0]['message']['content']
        else:  # claude
            content = data['content'][0]['text']
        
        # JSON ì¶”ì¶œ
        import re
        json_match = re.search(r'\{[\s\S]*\}', content)
        if json_match:
            ai_result = json.loads(json_match.group())
            
            # ê´€ë ¨ ì—†ìœ¼ë©´ None ë°˜í™˜
            if not ai_result.get('relevant', False):
                return None
            
            # ì„¸ë¶€ ì¹´í…Œê³ ë¦¬
            detail_category = ai_result.get('category', 'other')
            
            # ì •ì œëœ ì •ë³´ ë³‘í•©
            refined_item = {
                **news_item,
                'category': detail_category,  # ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ì €ì¥
                'category_group': map_to_group_category(detail_category),  # ê·¸ë£¹ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                'summary': ai_result.get('summary_kr', news_item.get('summary', '')),
                'traffic_impact': ai_result.get('traffic_impact', '')
            }
            
            # êµ­ê°€ ì •ë³´ ì—…ë°ì´íŠ¸
            if ai_result.get('country'):
                refined_item['country'] = ai_result.get('country')
                refined_item['continent'] = get_continent(ai_result.get('country'))
            
            return refined_item
        else:
            logger.warning("AI ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ì‚¬ìš©")
            return news_item
            
    except Exception as e:
        logger.error(f"AI ì •ì œ ì‹¤íŒ¨: {e}, ì›ë³¸ ë‰´ìŠ¤ ì‚¬ìš©")
        return news_item


def cross_validate_news(openai_news: List[Dict], claude_news: List[Dict]) -> List[Dict]:
    """
    OpenAIì™€ Claude API ê²°ê³¼ë¥¼ êµì°¨ê²€ì¦í•˜ì—¬ ì‹ ë¢°ë„ ë†’ì€ ë‰´ìŠ¤ ë°˜í™˜
    
    Args:
        openai_news: OpenAI APIë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤
        claude_news: Claude APIë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤
    
    Returns:
        êµì°¨ê²€ì¦ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ì‹ ë¢°ë„ ì ìˆ˜ í¬í•¨)
    """
    validated_news = []
    seen_titles = set()
    
    # ì œëª© ìœ ì‚¬ë„ ë¹„êµ í•¨ìˆ˜ (ê°„ë‹¨í•œ ë²„ì „)
    def title_similarity(title1: str, title2: str) -> float:
        """ë‘ ì œëª©ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0.0 ~ 1.0)"""
        title1_lower = title1.lower()
        title2_lower = title2.lower()
        
        # ì™„ì „ ì¼ì¹˜
        if title1_lower == title2_lower:
            return 1.0
        
        # ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ë„
        words1 = set(title1_lower.split())
        words2 = set(title2_lower.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    # OpenAI ë‰´ìŠ¤ ì²˜ë¦¬
    for news in openai_news:
        title = news.get('title', '').lower()
        if title in seen_titles:
            continue
        
        # Claude ê²°ê³¼ì™€ ë¹„êµ
        matched = False
        best_match_score = 0.0
        best_match = None
        
        for claude_item in claude_news:
            claude_title = claude_item.get('title', '').lower()
            similarity = title_similarity(title, claude_title)
            
            if similarity > 0.7:  # 70% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¼ì¹˜ë¡œ ê°„ì£¼
                matched = True
                if similarity > best_match_score:
                    best_match_score = similarity
                    best_match = claude_item
        
        if matched and best_match:
            # ë‘ APIê°€ ì¼ì¹˜í•˜ëŠ” ë‰´ìŠ¤: ì‹ ë¢°ë„ ë†’ìŒ
            validated_item = news.copy()
            validated_item['confidence'] = 'high'
            validated_item['validation'] = f"OpenAI + Claude ì¼ì¹˜ (ìœ ì‚¬ë„: {best_match_score:.0%})"
            validated_item['openai_summary'] = news.get('summary', '')
            validated_item['claude_summary'] = best_match.get('summary', '')
            # ë” ê¸´ ìš”ì•½ ì‚¬ìš©
            if len(best_match.get('summary', '')) > len(news.get('summary', '')):
                validated_item['summary'] = best_match.get('summary', '')
            validated_news.append(validated_item)
            seen_titles.add(title)
            seen_titles.add(best_match.get('title', '').lower())
        else:
            # OpenAIë§Œ ì°¾ì€ ë‰´ìŠ¤: ì‹ ë¢°ë„ ì¤‘ê°„
            validated_item = news.copy()
            validated_item['confidence'] = 'medium'
            validated_item['validation'] = 'OpenAI only'
            validated_news.append(validated_item)
            seen_titles.add(title)
    
    # Claudeë§Œ ì°¾ì€ ë‰´ìŠ¤ ì¶”ê°€
    for news in claude_news:
        title = news.get('title', '').lower()
        if title not in seen_titles:
            validated_item = news.copy()
            validated_item['confidence'] = 'medium'
            validated_item['validation'] = 'Claude only'
            validated_news.append(validated_item)
            seen_titles.add(title)
    
    # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (high > medium)
    validated_news.sort(key=lambda x: (x.get('confidence') == 'high', x.get('title', '')))
    
    logger.info(f"êµì°¨ê²€ì¦ ì™„ë£Œ: ì´ {len(validated_news)}ê°œ (High: {sum(1 for n in validated_news if n.get('confidence') == 'high')}, Medium: {sum(1 for n in validated_news if n.get('confidence') == 'medium')})")
    
    return validated_news


def fetch_news_with_cross_validation(keyword: str, countries: List[Dict] = None) -> List[Dict]:
    """
    OpenAIì™€ Claude APIë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ êµì°¨ê²€ì¦
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        countries: ê´€ë ¨ êµ­ê°€ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        êµì°¨ê²€ì¦ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"êµì°¨ê²€ì¦ ì‹œì‘: {keyword}")
    
    # ë‘ API ëª¨ë‘ í˜¸ì¶œ
    openai_news = fetch_news_from_openai(keyword, countries)
    time.sleep(1)  # API ë¶€í•˜ ë°©ì§€
    claude_news = fetch_news_from_claude(keyword, countries)
    
    # êµì°¨ê²€ì¦
    if openai_news or claude_news:
        validated = cross_validate_news(openai_news, claude_news)
        return validated
    else:
        # ë‘˜ ë‹¤ ì‹¤íŒ¨ ì‹œ RSS í´ë°±
        logger.warning("OpenAIì™€ Claude ëª¨ë‘ ì‹¤íŒ¨, RSSë¡œ í´ë°±")
        return fetch_news_from_rss(keyword)


def remove_duplicates(existing_news: List[Dict], new_news: List[Dict]) -> List[Dict]:
    """
    ì¤‘ë³µ ë‰´ìŠ¤ ì œê±° (URL ê¸°ì¤€)
    
    Args:
        existing_news: ê¸°ì¡´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        new_news: ìƒˆë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì¤‘ë³µ ì œê±°ëœ ìƒˆ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    existing_urls = {news['url'] for news in existing_news if news.get('url')}
    unique_news = [news for news in new_news if news.get('url') not in existing_urls]
    
    logger.info(f"ì¤‘ë³µ ì œê±°: {len(new_news)}ê°œ ì¤‘ {len(unique_news)}ê°œ ìœ ë‹ˆí¬")
    return unique_news


def load_existing_news() -> List[Dict]:
    """ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë¡œë“œ"""
    if not NEWS_CSV.exists():
        logger.info("ê¸°ì¡´ ë‰´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        return []
    
    try:
        df = pd.read_csv(NEWS_CSV, encoding='utf-8-sig')
        return df.to_dict('records')
    except Exception as e:
        logger.error(f"ê¸°ì¡´ ë‰´ìŠ¤ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return []


def save_to_csv(all_news: List[Dict]):
    """
    ë‰´ìŠ¤ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        all_news: ì €ì¥í•  ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    if not all_news:
        logger.warning("ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        DATA_DIR.mkdir(exist_ok=True)
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(all_news)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì§€ì • (news_type ë° êµì°¨ê²€ì¦ ì»¬ëŸ¼ í¬í•¨)
        base_columns = ['date', 'country', 'continent', 'title', 'summary', 'url', 'source', 'category', 'category_group', 'news_type', 'traffic_impact']
        optional_columns = ['priority', 'confidence', 'validation', 'openai_summary', 'claude_summary']
        
        # ëª¨ë“  ì»¬ëŸ¼ í™•ì¸
        all_columns = base_columns + [col for col in optional_columns if col in df.columns]
        df = df.reindex(columns=all_columns)
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date', ascending=False)
        df['date'] = df['date'].dt.strftime('%Y-%m-%d')
        
        # CSV ì €ì¥ (UTF-8 with BOM for Excel compatibility)
        df.to_csv(NEWS_CSV, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_MINIMAL)
        
        logger.info(f"ë‰´ìŠ¤ {len(all_news)}ê°œë¥¼ {NEWS_CSV}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        raise


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ì ìš©"""
    logger.info("=" * 50)
    logger.info("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (RSS + DeepSearch + AI ì •ì œ)")
    logger.info("=" * 50)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ API í™•ì¸
    apis_available = []
    if os.getenv('DEEPSEARCH_API_KEY'):
        apis_available.append('DeepSearch')
    if os.getenv('GROQ_API_KEY'):
        apis_available.append('Groq')
    if os.getenv('OPENAI_API_KEY'):
        apis_available.append('OpenAI')
    if os.getenv('CLAUDE_API_KEY') or os.getenv('ANTHROPIC_API_KEY'):
        apis_available.append('Claude')
    
    logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ API: {', '.join(apis_available) if apis_available else 'RSSë§Œ ì‚¬ìš©'}")
    
    try:
        # í‚¤ì›Œë“œ ë¡œë“œ
        keywords_config = load_keywords()
        base_keywords = keywords_config.get('base_keywords', [])
        gaming_keywords = keywords_config.get('gaming_keywords', {})
        priority_countries = keywords_config.get('priority_countries', {})
        traffic_impact_keywords = keywords_config.get('traffic_impact_keywords', {})
        
        # ê¸°ì¡´ ë‰´ìŠ¤ ë¡œë“œ
        existing_news = load_existing_news()
        logger.info(f"ê¸°ì¡´ ë‰´ìŠ¤: {len(existing_news)}ê°œ")
        
        all_raw_news = []
        
        # ============================================================
        # 0ë‹¨ê³„: DeepSearchë¡œ ê³ í’ˆì§ˆ ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì„ íƒì )
        # ============================================================
        if os.getenv('DEEPSEARCH_API_KEY'):
            logger.info("=" * 50)
            logger.info("0ë‹¨ê³„: DeepSearch ê³ í’ˆì§ˆ ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘...")
            logger.info("=" * 50)
            
            # íŠ¸ë˜í”½ ì˜í–¥ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
            deepsearch_keywords = [
                "internet shutdown", "power outage", "earthquake",
                "war conflict", "protest", "holiday", "gaming mobile"
            ]
            
            # ì£¼ìš” êµ­ê°€ ë¦¬ìŠ¤íŠ¸
            country_list = list(priority_countries.keys())
            
            deepsearch_news = fetch_from_deepsearch(deepsearch_keywords, country_list, max_results=30)
            all_raw_news.extend(deepsearch_news)
            
            # íŠ¸ë Œë”© í† í”½ë„ ìˆ˜ì§‘
            trending_news = fetch_trending_from_deepsearch(['world', 'technology'])
            all_raw_news.extend(trending_news)
            
            logger.info(f"DeepSearch ìˆ˜ì§‘ ì™„ë£Œ: {len(deepsearch_news) + len(trending_news)}ê°œ")
        
        # ============================================================
        # 0-Bë‹¨ê³„: ë„¤ì´ë²„ ê²€ìƒ‰ APIë¡œ êµ­ë‚´ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì„ íƒì )
        # ============================================================
        if os.getenv('NAVER_CLIENT_ID') and os.getenv('NAVER_CLIENT_SECRET'):
            logger.info("=" * 50)
            logger.info("0-Bë‹¨ê³„: ë„¤ì´ë²„ êµ­ë‚´ ë‰´ìŠ¤ ìˆ˜ì§‘...")
            logger.info("=" * 50)
            
            # êµ­ë‚´ ë‰´ìŠ¤ ê²€ìƒ‰ í‚¤ì›Œë“œ
            naver_keywords = [
                "PUBG ëª¨ë°”ì¼", "íì§€ ëª¨ë°”ì¼", "ë°°í‹€ê·¸ë¼ìš´ë“œ ëª¨ë°”ì¼",
                "í¬ë˜í”„í†¤", "ëª¨ë°”ì¼ ê²Œì„",
                "ì¸í„°ë„· ì¥ì• ", "í†µì‹  ì¥ì• ",
                "ì§€ì§„ ì†ë³´", "íƒœí’ ì†ë³´"
            ]
            
            naver_news = fetch_from_naver(naver_keywords, max_results=30)
            all_raw_news.extend(naver_news)
            
            logger.info(f"ë„¤ì´ë²„ ìˆ˜ì§‘ ì™„ë£Œ: {len(naver_news)}ê°œ")
        
        # ============================================================
        # 1ë‹¨ê³„: ê²Œì„ ë‰´ìŠ¤ ìˆ˜ì§‘ (gaming_keywords)
        # ============================================================
        logger.info("=" * 50)
        logger.info("1ë‹¨ê³„: ê²Œì„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        logger.info("=" * 50)
        
        # ê¸°ë³¸ PUBG í‚¤ì›Œë“œ
        for keyword in base_keywords:
            news = fetch_news_from_rss(keyword)
            for item in news:
                item['news_type'] = 'gaming'
            all_raw_news.extend(news)
            time.sleep(0.5)
        
        # ê²Œì„ í‚¤ì›Œë“œ (ê° ì¹´í…Œê³ ë¦¬ë‹¹ 2ê°œì”©)
        for category, keywords in gaming_keywords.items():
            for keyword in keywords[:2]:
                news = fetch_news_from_rss(keyword)
                for item in news:
                    item['news_type'] = 'gaming'
                    item['category'] = 'gaming' if category in ['pubg', 'krafton', 'esports'] else 'competitor_game'
                all_raw_news.extend(news)
                time.sleep(0.5)
        
        gaming_count = len(all_raw_news)
        logger.info(f"ê²Œì„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {gaming_count}ê°œ")
        
        # ============================================================
        # 2ë‹¨ê³„: íŠ¸ë˜í”½ ì˜í–¥ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì£¼ìš” êµ­ê°€ + ìœ„ê¸° í‚¤ì›Œë“œ)
        # ============================================================
        logger.info("=" * 50)
        logger.info("2ë‹¨ê³„: íŠ¸ë˜í”½ ì˜í–¥ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        logger.info("=" * 50)
        
        # ì£¼ìš” êµ­ê°€ë³„ ê²€ìƒ‰
        for country, country_info in priority_countries.items():
            # êµ­ê°€ í‚¤ì›Œë“œ (ìµœëŒ€ 1ê°œ)
            for keyword in country_info.get('keywords', [])[:1]:
                news = fetch_news_from_rss(keyword)
                for item in news:
                    item['country'] = country
                    item['continent'] = get_continent(country)
                all_raw_news.extend(news)
                time.sleep(0.5)
            
            # êµ­ê°€ë³„ ì£¼ì œ í‚¤ì›Œë“œ (ìµœëŒ€ 2ê°œ)
            for topic in country_info.get('topics', [])[:2]:
                keyword = f"{country} {topic}"
                news = fetch_news_from_rss(keyword)
                for item in news:
                    item['country'] = country
                    item['continent'] = get_continent(country)
                all_raw_news.extend(news)
                time.sleep(0.5)
        
        # íŠ¸ë˜í”½ ì˜í–¥ í‚¤ì›Œë“œ (ê° ì¹´í…Œê³ ë¦¬ë‹¹ 1ê°œ, ì´ 15ê°œ)
        keyword_count = 0
        max_traffic_keywords = 15
        for category, keywords in traffic_impact_keywords.items():
            if keyword_count >= max_traffic_keywords:
                break
            for keyword in keywords[:1]:
                if keyword_count >= max_traffic_keywords:
                    break
                news = fetch_news_from_rss(keyword)
                all_raw_news.extend(news)
                keyword_count += 1
                time.sleep(0.5)
        
        traffic_count = len(all_raw_news) - gaming_count
        logger.info(f"íŠ¸ë˜í”½ ì˜í–¥ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {traffic_count}ê°œ")
        logger.info(f"ì´ RSS ìˆ˜ì§‘: {len(all_raw_news)}ê°œ")
        
        # ============================================================
        # 3ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ë° ë¶„ë¥˜
        # ============================================================
        logger.info("=" * 50)
        logger.info("3ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ì¤‘...")
        logger.info("=" * 50)
        
        high_priority_news = []
        medium_priority_news = []
        
        for news_item in all_raw_news:
            priority = news_item.get('priority', 'medium')
            
            if priority == 'high':
                high_priority_news.append(news_item)
            else:
                # MEDIUM: ê·œì¹™ ê¸°ë°˜ ìë™ ë¶„ë¥˜ ì™„ë£Œ
                if not news_item.get('category_group'):
                    news_item['category_group'] = map_to_group_category(news_item.get('category', 'gaming'))
                medium_priority_news.append(news_item)
        
        logger.info(f"HIGH Priority (AI ì •ì œ ëŒ€ìƒ): {len(high_priority_news)}ê°œ")
        logger.info(f"MEDIUM Priority (ê·œì¹™ ê¸°ë°˜): {len(medium_priority_news)}ê°œ")
        
        # ============================================================
        # 4ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ AI ì •ì œ (ë¬´ë£Œ API ìš°ì„ )
        # ============================================================
        logger.info("=" * 50)
        logger.info("4ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ AI ì •ì œ ì‹œì‘...")
        logger.info("=" * 50)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ API í™•ì¸
        available_apis = []
        if os.getenv('GROQ_API_KEY'):
            available_apis.append('Groq (ë¬´ë£Œ)')
        if os.getenv('GEMINI_API_KEY') or os.getenv('GOOGLE_API_KEY'):
            available_apis.append('Gemini (ë¬´ë£Œ)')
        if os.getenv('OPENAI_API_KEY'):
            available_apis.append('OpenAI (ìœ ë£Œ)')
        if os.getenv('CLAUDE_API_KEY') or os.getenv('ANTHROPIC_API_KEY'):
            available_apis.append('Claude (ìœ ë£Œ)')
        
        if available_apis:
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ API: {', '.join(available_apis)}")
        else:
            logger.info("API í‚¤ ì—†ìŒ - ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ë§Œ ì‚¬ìš©")
        
        # ìœ ë£Œ API ì‚¬ìš© ì—¬ë¶€ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´)
        use_paid = os.getenv('USE_PAID_API', 'false').lower() == 'true'
        
        # ìŠ¤ë§ˆíŠ¸ ì •ì œ ì‹¤í–‰ (HIGH Priorityë§Œ)
        all_refined_news = smart_refine_batch(high_priority_news, use_paid_api=use_paid)
        
        # skip í‘œì‹œëœ ë‰´ìŠ¤ ì œê±°
        all_refined_news = [n for n in all_refined_news if not n.get('skip')]
        
        # MEDIUM Priority ë‰´ìŠ¤ ì¶”ê°€ (ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜)
        for item in medium_priority_news:
            if not item.get('category_group'):
                item['category_group'] = map_to_group_category(item.get('category', 'gaming'))
        all_refined_news.extend(medium_priority_news)
        
        logger.info(f"ì´ ì²˜ë¦¬ëœ ë‰´ìŠ¤: {len(all_refined_news)}ê°œ")
        
        # ë‰´ìŠ¤ íƒ€ì…ë³„ í†µê³„
        gaming_final = sum(1 for n in all_refined_news if n.get('news_type') == 'gaming')
        traffic_final = sum(1 for n in all_refined_news if n.get('news_type') == 'traffic_impact')
        logger.info(f"  - ğŸ® ê²Œì„ ë‰´ìŠ¤: {gaming_final}ê°œ")
        logger.info(f"  - âš¡ íŠ¸ë˜í”½ ì˜í–¥ ë‰´ìŠ¤: {traffic_final}ê°œ")
        
        # ì¤‘ë³µ ì œê±°
        unique_new_news = remove_duplicates(existing_news, all_refined_news)
        
        # ê¸°ì¡´ ë‰´ìŠ¤ì™€ í•©ì¹˜ê¸°
        all_news = existing_news + unique_new_news
        
        # ì €ì¥
        if unique_new_news:
            save_to_csv(all_news)
            logger.info(f"âœ… {len(unique_new_news)}ê°œì˜ ìƒˆ ë‰´ìŠ¤ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
            
            # ============================================================
            # 5ë‹¨ê³„: AI ìš”ì•½ ìƒì„±
            # ============================================================
            logger.info("=" * 50)
            logger.info("5ë‹¨ê³„: íŠ¸ë˜í”½ ì˜í–¥ AI ìš”ì•½ ìƒì„±...")
            logger.info("=" * 50)
            
            # ìµœê·¼ 24ì‹œê°„ ë‰´ìŠ¤ë§Œ ìš”ì•½ ëŒ€ìƒ
            now = datetime.now()
            recent_news = [
                n for n in all_news 
                if n.get('published_date') and 
                (now - datetime.fromisoformat(n['published_date'].replace('Z', '+00:00').replace('+00:00', ''))).days < 1
            ]
            
            if not recent_news:
                recent_news = all_news[:50]  # fallback
            
            generate_traffic_summary(recent_news)
            
            return 0  # ì„±ê³µ (ë³€ê²½ì‚¬í•­ ìˆìŒ)
        else:
            logger.info("ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë‰´ìŠ¤ê°€ ì—†ì–´ë„ ìš”ì•½ì€ ê°±ì‹ 
            logger.info("ê¸°ì¡´ ë‰´ìŠ¤ë¡œ ìš”ì•½ ê°±ì‹ ...")
            generate_traffic_summary(existing_news[:50] if existing_news else [])
            
            return 1  # ë³€ê²½ì‚¬í•­ ì—†ìŒ
            
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return -1  # ì‹¤íŒ¨


def generate_traffic_summary(news_list: List[Dict]) -> Dict:
    """
    íŠ¸ë˜í”½ ì˜í–¥ ë‰´ìŠ¤ì— ëŒ€í•œ AI ìš”ì•½ ìƒì„±
    - ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ ì „ë¬¸ê°€ ê´€ì 
    - Groq API ì‚¬ìš© (ë¬´ë£Œ)
    """
    SUMMARY_FILE = DATA_DIR / 'summary.json'
    
    # íŠ¸ë˜í”½ ì˜í–¥ ë‰´ìŠ¤ë§Œ í•„í„°ë§
    traffic_news = [n for n in news_list if n.get('news_type') == 'traffic_impact']
    
    # ì œì™¸ í‚¤ì›Œë“œ (íŠ¸ë˜í”½ê³¼ ë¬´ê´€í•œ ë‰´ìŠ¤)
    exclude_keywords = [
        'mama', 'awards', 'ì‹œìƒì‹', 'concert', 'ì½˜ì„œíŠ¸', 'idol', 'ì•„ì´ëŒ',
        'ê´‘ê³ ', 'ìº í˜ì¸', 'í”„ë¡œëª¨ì…˜', 'ì¦ì‹œ', 'ì½”ìŠ¤í”¼', 'ì±„ìš©', 'ë¶„ì–‘',
        'immigration protest', 'hindu protest', 'farmer protest'
    ]
    
    # í•„í„°ë§ëœ ë‰´ìŠ¤
    filtered_news = []
    seen_titles = set()
    for news in traffic_news:
        title = (news.get('title') or '').lower()
        title_key = title[:30]
        
        # ì¤‘ë³µ ì²´í¬
        if title_key in seen_titles:
            continue
        seen_titles.add(title_key)
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        if any(kw in title for kw in exclude_keywords):
            continue
            
        filtered_news.append(news)
    
    logger.info(f"ìš”ì•½ ëŒ€ìƒ ë‰´ìŠ¤: {len(filtered_news)}ê°œ (ì „ì²´ íŠ¸ë˜í”½ ë‰´ìŠ¤: {len(traffic_news)}ê°œ)")
    
    if not filtered_news:
        summary_data = {
            'generated_at': datetime.now().isoformat(),
            'news_count': 0,
            'has_issues': False,
            'summary': 'âœ… íŠ¹ì´ì‚¬í•­ ì—†ìŒ\n\nìµœê·¼ 24ì‹œê°„ ë™ì•ˆ ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ì— ì˜í–¥ì„ ì¤„ ë§Œí•œ ì£¼ìš” ì´ìŠˆê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
            'affected_countries': [],
            'key_issues': []
        }
        
        with open(SUMMARY_FILE, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        logger.info("ìš”ì•½ ìƒì„± ì™„ë£Œ: íŠ¹ì´ì‚¬í•­ ì—†ìŒ")
        return summary_data
    
    # ìœ íš¨í•œ êµ­ê°€ë§Œ ì¶”ì¶œ (NaN, None, 'Unknown' ì œì™¸)
    def get_valid_countries(news_list):
        countries = []
        for n in news_list:
            country = n.get('country')
            if country and str(country) not in ['nan', 'NaN', 'Unknown', 'None', '']:
                countries.append(country)
        return list(set(countries))
    
    # Groq APIë¡œ ìš”ì•½ ìƒì„±
    api_key = os.getenv('GROQ_API_KEY')
    if not api_key:
        logger.warning("GROQ_API_KEY ì—†ìŒ - ê¸°ë³¸ ìš”ì•½ ìƒì„±")
        summary_data = {
            'generated_at': datetime.now().isoformat(),
            'news_count': len(filtered_news),
            'has_issues': True,
            'summary': f'íŠ¸ë˜í”½ ì˜í–¥ ë‰´ìŠ¤ {len(filtered_news)}ê±´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'affected_countries': get_valid_countries(filtered_news),
            'key_issues': [{'title': n.get('title', '')[:80], 'country': n.get('country', '')} for n in filtered_news[:5]]
        }
        
        with open(SUMMARY_FILE, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        return summary_data
    
    try:
        import requests
        
        # ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ìµœëŒ€ 5ê°œë¡œ ì¤„ì„)
        news_items = []
        for n in filtered_news[:5]:
            country = n.get('country', '')
            if str(country) in ['nan', 'NaN', 'Unknown', 'None', '']:
                country = 'ê¸€ë¡œë²Œ'
            title = n.get('title', '')[:60]
            news_items.append(f"[{country}] {title}")
        
        news_text = "\n".join(news_items)
        
        prompt = f"""ì•„ë˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•´ ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ ì˜í–¥ì„ ìš”ì•½í•´ì¤˜.

{news_text}

ìš”ì²­:
1. í•µì‹¬ ì˜í–¥ 2-3ë¬¸ì¥
2. êµ­ê°€ë³„ ì˜ˆìƒ ì˜í–¥ 1ë¬¸ì¥ì”©
3. ê°„ê²°í•œ í•œêµ­ì–´ë¡œ"""

        response = requests.post(
            "https://api.groq.com/openai/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "llama-3.3-70b-versatile",
                "messages": [
                    {"role": "system", "content": "ëª¨ë°”ì¼ ê²Œì„ íŠ¸ë˜í”½ ì „ë¬¸ê°€. ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3,
                "max_tokens": 500
            },
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            ai_summary = data['choices'][0]['message']['content']
            
            # ì˜í–¥ êµ­ê°€ ì¶”ì¶œ (ìœ íš¨í•œ êµ­ê°€ë§Œ)
            affected_countries = get_valid_countries(filtered_news)
            
            # key_issues ìƒì„± (NaN ì²˜ë¦¬)
            key_issues = []
            for n in filtered_news[:5]:
                country = n.get('country', '')
                if str(country) in ['nan', 'NaN', 'Unknown', 'None', '']:
                    country = ''
                key_issues.append({
                    'title': n.get('title', '')[:80],
                    'country': country,
                    'category': n.get('category', 'other')
                })
            
            summary_data = {
                'generated_at': datetime.now().isoformat(),
                'news_count': len(filtered_news),
                'has_issues': True,
                'summary': ai_summary,
                'affected_countries': affected_countries,
                'key_issues': key_issues
            }
            
            logger.info("AI ìš”ì•½ ìƒì„± ì™„ë£Œ")
        else:
            logger.warning(f"Groq API ì˜¤ë¥˜: {response.status_code} - {response.text[:200]}")
            
            # key_issues ìƒì„± (NaN ì²˜ë¦¬)
            key_issues = []
            for n in filtered_news[:5]:
                country = n.get('country', '')
                if str(country) in ['nan', 'NaN', 'Unknown', 'None', '']:
                    country = ''
                key_issues.append({
                    'title': n.get('title', '')[:80],
                    'country': country
                })
            
            summary_data = {
                'generated_at': datetime.now().isoformat(),
                'news_count': len(filtered_news),
                'has_issues': True,
                'summary': f'íŠ¸ë˜í”½ ì˜í–¥ ë‰´ìŠ¤ {len(filtered_news)}ê±´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. (API ì˜¤ë¥˜ë¡œ ìƒì„¸ ë¶„ì„ ë¶ˆê°€)',
                'affected_countries': get_valid_countries(filtered_news),
                'key_issues': key_issues
            }
    
    except Exception as e:
        logger.error(f"ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
        summary_data = {
            'generated_at': datetime.now().isoformat(),
            'news_count': len(filtered_news),
            'has_issues': True,
            'summary': f'íŠ¸ë˜í”½ ì˜í–¥ ë‰´ìŠ¤ {len(filtered_news)}ê±´ ê°ì§€ë¨. (ìš”ì•½ ìƒì„± ì˜¤ë¥˜)',
            'affected_countries': [],
            'key_issues': []
        }
    
    # ì €ì¥
    with open(SUMMARY_FILE, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ìš”ì•½ ì €ì¥ ì™„ë£Œ: {SUMMARY_FILE}")
    return summary_data


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)

